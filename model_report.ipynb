{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b14f4ee",
   "metadata": {},
   "source": [
    "# Model Report — Training Curves and Statistics\n",
    "\n",
    "This notebook summarizes the trained model (`models/best_model.pt`), loads metrics from `logs/`, plots training curves, and displays the confusion matrix if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b70c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\n",
      "Checkpoint: C:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\models\\best_model.pt\n",
      "Logs dir: C:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\logs\n",
      "Report out dir: C:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\logs\\report\n"
     ]
    }
   ],
   "source": [
    "# 1) Configure Paths and Run Context\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Resolve project root (this notebook is under notebooks/)\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0] if (Path.cwd().name == 'notebooks') else Path.cwd().resolve()\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "\n",
    "# Ensure both src/ and project root are importable\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import config and derive key paths\n",
    "import config\n",
    "importlib.reload(config)\n",
    "CKPT_PATH = config.MODELS_DIR / 'best_model.pt'\n",
    "LOGS_DIR = config.LOGS_DIR\n",
    "OUT_DIR = LOGS_DIR / 'report'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Checkpoint:', CKPT_PATH)\n",
    "print('Logs dir:', LOGS_DIR)\n",
    "print('Report out dir:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcfb6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# 2) Install and Import Dependencies\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Torch:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18310a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: ['epoch', 'model_state_dict', 'optimizer_state_dict', 'val_acc', 'val_loss']\n"
     ]
    }
   ],
   "source": [
    "# 3) Load best_model.pt Checkpoint\n",
    "ckpt = None\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = torch.load(CKPT_PATH, map_location='cpu')\n",
    "    if isinstance(ckpt, dict):\n",
    "        print('Checkpoint keys:', list(ckpt.keys())[:10])\n",
    "    else:\n",
    "        print('Checkpoint is a state_dict-like object')\n",
    "else:\n",
    "    print(f'⚠ Checkpoint not found at {CKPT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96c248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 423,760\n",
      "Checkpoint size (MB): 4.88\n",
      "Dtype counts: {'torch.float32': 28, 'torch.int64': 4}\n"
     ]
    }
   ],
   "source": [
    "# 4) Summarize Model and Parameter Count\n",
    "from typing import Any\n",
    "param_count = None\n",
    "file_size_mb = None\n",
    "dtype_counts = {}\n",
    "\n",
    "if CKPT_PATH.exists():\n",
    "    try:\n",
    "        from model import create_model\n",
    "        # Recreate model according to config\n",
    "        model = create_model(n_mels=config.MODEL_CONFIG['input_shape'][0],\n",
    "                             n_classes=config.MODEL_CONFIG['n_classes'])\n",
    "        sd = ckpt.get('model_state_dict', ckpt if isinstance(ckpt, dict) else {})\n",
    "        if isinstance(sd, dict):\n",
    "            try: model.load_state_dict(sd, strict=False)\n",
    "            except Exception as e: print('Load state_dict warning:', e)\n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        print('Model parameters:', f'{param_count:,}')\n",
    "    except Exception as e:\n",
    "        # Fallback: count parameters from state_dict tensors\n",
    "        sd = ckpt.get('model_state_dict', ckpt if isinstance(ckpt, dict) else {})\n",
    "        if isinstance(sd, dict):\n",
    "            param_count = int(sum(v.numel() for v in sd.values()))\n",
    "            print('Parameter count (state_dict sum):', f'{param_count:,}')\n",
    "        else:\n",
    "            print('Could not infer parameter count.')\n",
    "    try:\n",
    "        file_size_mb = os.path.getsize(CKPT_PATH) / (1024*1024)\n",
    "        print('Checkpoint size (MB):', f'{file_size_mb:.2f}')\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Dtype distribution\n",
    "    try:\n",
    "        sd = ckpt.get('model_state_dict', ckpt if isinstance(ckpt, dict) else {})\n",
    "        for k,v in sd.items():\n",
    "            dt = str(getattr(v, 'dtype', 'unknown'))\n",
    "            dtype_counts[dt] = dtype_counts.get(dt, 0) + 1\n",
    "        print('Dtype counts:', dtype_counts)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d59f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>62.553062</td>\n",
       "      <td>1.301814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch    val_acc  val_loss\n",
       "0      0  62.553062  1.301814"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5) Extract Training Metadata from Checkpoint\n",
    "import pandas as pd\n",
    "meta_fields = ['epoch','val_acc','val_loss','best_metric','best_epoch','train_time','git_sha']\n",
    "rows = {}\n",
    "if isinstance(ckpt, dict):\n",
    "    for k in meta_fields:\n",
    "        if k in ckpt:\n",
    "            rows[k] = ckpt[k]\n",
    "if rows:\n",
    "    display(pd.DataFrame([rows]))\n",
    "else:\n",
    "    print('No training metadata found in checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb3f15d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TensorBoard scalars found.\n"
     ]
    }
   ],
   "source": [
    "# 6) Load Metrics: TensorBoard Event Files (optional)\n",
    "from glob import glob\n",
    "tb_df = None\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    event_files = glob(str(LOGS_DIR / '**' / 'events.out.tfevents.*'), recursive=True)\n",
    "    records = []\n",
    "    for ef in event_files:\n",
    "        try:\n",
    "            ea = EventAccumulator(ef)\n",
    "            ea.Reload()\n",
    "            for tag in ['train/loss','val/loss','train/acc','val/acc','lr']:\n",
    "                if tag in ea.Tags().get('scalars', []):\n",
    "                    for w in ea.Scalars(tag):\n",
    "                        records.append({'run': os.path.basename(os.path.dirname(ef)), 'tag': tag, 'step': w.step, 'wall_time': w.wall_time, 'value': w.value})\n",
    "        except Exception:\n",
    "            pass\n",
    "    if records:\n",
    "        tb_df = pd.DataFrame(records)\n",
    "        print('Loaded TensorBoard scalars:', len(tb_df))\n",
    "    else:\n",
    "        print('No TensorBoard scalars found.')\n",
    "except Exception as e:\n",
    "    print('TensorBoard not available or failed to load:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "838d8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Load Metrics: history.csv / metrics.csv / metrics.json\n",
    "hist = None\n",
    "hist_candidates = [LOGS_DIR / 'training_metrics.csv', LOGS_DIR / 'history.csv', PROJECT_ROOT / 'history.csv']\n",
    "for p in hist_candidates:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            hist = pd.read_csv(p)\n",
    "            print('Loaded metrics table from', p)\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "metrics_json = None\n",
    "metrics_json_path = LOGS_DIR / 'metrics.json'\n",
    "if metrics_json_path.exists():\n",
    "    try:\n",
    "        with open(metrics_json_path, 'r') as f:\n",
    "            metrics_json = json.load(f)\n",
    "        print('Loaded evaluation metrics from', metrics_json_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Normalize column names\n",
    "if hist is not None:\n",
    "    cols = {c.lower().strip(): c for c in hist.columns}\n",
    "    rename = {}\n",
    "    for key in ['epoch','step','train_loss','val_loss','train_acc','val_acc','lr']:\n",
    "        if key in cols:\n",
    "            rename[cols[key]] = key\n",
    "    hist = hist.rename(columns=rename)\n",
    "    if 'epoch' not in hist.columns and 'step' in hist.columns:\n",
    "        hist['epoch'] = hist['step']\n",
    "    if 'step' not in hist.columns and 'epoch' in hist.columns:\n",
    "        hist['step'] = hist['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6af9bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No history table available for merge.\n"
     ]
    }
   ],
   "source": [
    "# 8) Merge and Clean Metrics (+ EMA smoothing)\n",
    "merged = None\n",
    "if hist is not None and not hist.empty:\n",
    "    merged = hist.copy()\n",
    "    merged = merged.sort_values(by=['epoch']).reset_index(drop=True)\n",
    "    # EMA smoothing for curves\n",
    "    def ema(series, alpha=0.2):\n",
    "        out = []\n",
    "        prev = None\n",
    "        for x in series:\n",
    "            prev = (alpha * x + (1-alpha) * prev) if prev is not None else x\n",
    "            out.append(prev)\n",
    "        return out\n",
    "    for col in ['train_loss','val_loss','train_acc','val_acc']:\n",
    "        if col in merged.columns:\n",
    "            merged[f'{col}_ema'] = ema(merged[col].values, alpha=0.2)\n",
    "    display(merged.head())\n",
    "else:\n",
    "    print('No history table available for merge.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfebfdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No merged metrics to plot.\n"
     ]
    }
   ],
   "source": [
    "# 9) Plot Training/Validation Loss and Accuracy\n",
    "if merged is not None and not merged.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n",
    "    x = merged['epoch'] if 'epoch' in merged else range(len(merged))\n",
    "    # Loss\n",
    "    if 'train_loss' in merged: ax1.plot(x, merged['train_loss'], label='train_loss', alpha=0.6)\n",
    "    if 'val_loss' in merged: ax1.plot(x, merged['val_loss'], label='val_loss', alpha=0.8)\n",
    "    if 'train_loss_ema' in merged: ax1.plot(x, merged['train_loss_ema'], label='train_loss_ema', lw=2)\n",
    "    if 'val_loss_ema' in merged: ax1.plot(x, merged['val_loss_ema'], label='val_loss_ema', lw=2)\n",
    "    ax1.set_title('Loss'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    # Accuracy\n",
    "    if 'train_acc' in merged: ax2.plot(x, merged['train_acc'], label='train_acc', alpha=0.6)\n",
    "    if 'val_acc' in merged: ax2.plot(x, merged['val_acc'], label='val_acc', alpha=0.8)\n",
    "    if 'train_acc_ema' in merged: ax2.plot(x, merged['train_acc_ema'], label='train_acc_ema', lw=2)\n",
    "    if 'val_acc_ema' in merged: ax2.plot(x, merged['val_acc_ema'], label='val_acc_ema', lw=2)\n",
    "    ax2.set_title('Accuracy (%)'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy'); ax2.legend(); ax2.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(OUT_DIR / 'training_curves.png', dpi=200)\n",
    "    plt.show()\n",
    "    print('Saved:', OUT_DIR / 'training_curves.png')\n",
    "else:\n",
    "    print('No merged metrics to plot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "076938a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics to identify best epoch.\n"
     ]
    }
   ],
   "source": [
    "# 10) Identify Best Epoch and Annotate\n",
    "best_epoch = None\n",
    "text = ''\n",
    "if merged is not None and not merged.empty:\n",
    "    if 'val_loss' in merged:\n",
    "        best_epoch = int(merged['val_loss'].idxmin()) + 1\n",
    "        text = f'Best by val_loss: epoch {best_epoch}, val_loss={merged.loc[best_epoch-1, \"val_loss\"]:.4f}'\n",
    "    elif 'val_acc' in merged:\n",
    "        best_epoch = int(merged['val_acc'].idxmax()) + 1\n",
    "        text = f'Best by val_acc: epoch {best_epoch}, val_acc={merged.loc[best_epoch-1, \"val_acc\"]:.2f}%'\n",
    "    print(text)\n",
    "else:\n",
    "    print('No metrics to identify best epoch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013b1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR not available.\n"
     ]
    }
   ],
   "source": [
    "# 11) Additional Stats and Learning Rate Schedule\n",
    "if merged is not None and not merged.empty and 'lr' in merged:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(merged['epoch'], merged['lr'], label='lr')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Learning Rate'); plt.title('LR Schedule')\n",
    "    plt.grid(True, alpha=0.3); plt.legend();\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / 'lr_schedule.png', dpi=200)\n",
    "    plt.show()\n",
    "    print('Saved:', OUT_DIR / 'lr_schedule.png')\n",
    "else:\n",
    "    print('LR not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0f51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optional: Re-evaluation placeholder. Use evaluate.py for full evaluation.\n"
     ]
    }
   ],
   "source": [
    "# 12) Optional: Re-evaluate Best Model on Validation Set\n",
    "print('Optional: Re-evaluation placeholder. Use evaluate.py for full evaluation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2286226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No confusion matrix found in logs/.\n"
     ]
    }
   ],
   "source": [
    "# 13) Optional: Confusion Matrix and ROC/PR Curves\n",
    "from IPython.display import Image as IPyImage, display\n",
    "cm_candidates = [\n",
    "    LOGS_DIR / 'confusion_matrix.png',\n",
    "    LOGS_DIR / 'confusion-matrix.png',\n",
    "    LOGS_DIR / 'confusion_matrix.jpg',\n",
    "    LOGS_DIR / 'confusion-matrix.jpg',\n",
    "]\n",
    "cm_path = next((p for p in cm_candidates if p.exists()), None)\n",
    "if cm_path:\n",
    "    print('Confusion Matrix:')\n",
    "    display(IPyImage(filename=str(cm_path)))\n",
    "else:\n",
    "    print('No confusion matrix found in logs/.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc16fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No merged metrics to export.\n"
     ]
    }
   ],
   "source": [
    "# 14) Save Figures and Export Report\n",
    "# Save merged metrics to CSV and write a small summary JSON\n",
    "if merged is not None and not merged.empty:\n",
    "    merged.to_csv(OUT_DIR / 'merged_metrics.csv', index=False)\n",
    "    summary = {\n",
    "        'checkpoint': str(CKPT_PATH),\n",
    "        'params': int(param_count) if param_count is not None else None,\n",
    "        'checkpoint_size_mb': float(f'{(os.path.getsize(CKPT_PATH)/(1024*1024)):.4f}') if CKPT_PATH.exists() else None,\n",
    "        'best_epoch_text': text,\n",
    "    }\n",
    "    with open(OUT_DIR / 'summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print('Saved:', OUT_DIR / 'merged_metrics.csv')\n",
    "    print('Saved:', OUT_DIR / 'summary.json')\n",
    "else:\n",
    "    print('No merged metrics to export.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
