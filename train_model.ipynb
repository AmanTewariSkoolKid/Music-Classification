{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca073aaa",
   "metadata": {},
   "source": [
    "# Music Genre Classification - Training Notebook\n",
    "\n",
    "This notebook combines data loading from FMA dataset and model training into a single workflow.\n",
    "\n",
    "## Steps:\n",
    "1. Load FMA metadata and audio files\n",
    "2. Create train/validation splits\n",
    "3. Preprocess audio to log-mel spectrograms\n",
    "4. Train CNN model for genre classification\n",
    "5. Evaluate and save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec21e8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08829ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "PyTorch version: 2.1.0+cpu\n",
      "CUDA available: False\n",
      "Expecting metadata at: c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_metadata\\tracks.csv\n",
      "Expecting audio root at: c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_medium\n",
      "✓ Dataset structure verified\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Ensure src directory is on sys.path for module imports\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reload config to pick up any recent edits (e.g., path changes)\n",
    "import config\n",
    "importlib.reload(config)\n",
    "TRAINING_CONFIG = config.TRAINING_CONFIG\n",
    "MODEL_CONFIG = config.MODEL_CONFIG\n",
    "GENRE_LABELS = config.GENRE_LABELS\n",
    "MODELS_DIR = config.MODELS_DIR\n",
    "DATA_DIR = config.DATA_DIR\n",
    "\n",
    "from model import create_model\n",
    "from preprocessing import load_and_preprocess_audio, spec_augment\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Validate critical data paths early\n",
    "tracks_csv = DATA_DIR / \"fma_metadata\" / \"tracks.csv\"\n",
    "audio_root = DATA_DIR / \"fma_medium\"\n",
    "print(f\"Expecting metadata at: {tracks_csv}\")\n",
    "print(f\"Expecting audio root at: {audio_root}\")\n",
    "if not tracks_csv.exists():\n",
    "    raise FileNotFoundError(f\"Missing tracks.csv at {tracks_csv}. Ensure dataset extracted to 'data/'.\")\n",
    "if not audio_root.exists():\n",
    "    raise FileNotFoundError(f\"Missing fma_medium audio directory at {audio_root}.\")\n",
    "print(\"✓ Dataset structure verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e028a",
   "metadata": {},
   "source": [
    "## 2. Load FMA Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fe855b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from: c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_metadata\\tracks.csv\n",
      "✓ Loaded 106574 tracks\n",
      "Columns: [('album', 'comments'), ('album', 'date_created'), ('album', 'date_released'), ('album', 'engineer'), ('album', 'favorites')]...\n",
      "✓ Loaded 106574 tracks\n",
      "Columns: [('album', 'comments'), ('album', 'date_created'), ('album', 'date_released'), ('album', 'engineer'), ('album', 'favorites')]...\n"
     ]
    }
   ],
   "source": [
    "# Load FMA metadata with multi-level headers\n",
    "metadata_path = DATA_DIR / \"fma_metadata\" / \"tracks.csv\"\n",
    "print(f\"Loading metadata from: {metadata_path}\")\n",
    "\n",
    "# FMA tracks.csv has multi-level column headers\n",
    "tracks = pd.read_csv(metadata_path, index_col=0, header=[0, 1])\n",
    "\n",
    "print(f\"✓ Loaded {len(tracks)} tracks\")\n",
    "print(f\"Columns: {tracks.columns.tolist()[:5]}...\")  # Show first few columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20be3ce",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset - Filter by Genres and Create File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b3d93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks in medium subset: 17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tracks: 100%|██████████| 17000/17000 [00:04<00:00, 3860.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created dataset with 16490 samples\n",
      "\n",
      "Genre distribution:\n",
      "genre\n",
      "Rock              6103\n",
      "Electronic        5314\n",
      "Experimental      1251\n",
      "Hip-Hop           1201\n",
      "Classical          619\n",
      "Folk               519\n",
      "Jazz               384\n",
      "Instrumental       350\n",
      "Pop                186\n",
      "Country            178\n",
      "Soul-RnB           154\n",
      "Spoken             118\n",
      "Blues               74\n",
      "Easy Listening      21\n",
      "International       18\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the genre information (top level genre)\n",
    "genre_top = tracks[('track', 'genre_top')]\n",
    "\n",
    "# Filter to only medium subset tracks\n",
    "subset = tracks[('set', 'subset')]\n",
    "medium_tracks = tracks[subset == 'medium']\n",
    "\n",
    "print(f\"Tracks in medium subset: {len(medium_tracks)}\")\n",
    "\n",
    "# Create dataset from medium tracks with valid genres\n",
    "audio_dir = DATA_DIR / \"fma_medium\"\n",
    "samples = []\n",
    "\n",
    "for track_id in tqdm(medium_tracks.index, desc=\"Processing tracks\"):\n",
    "    try:\n",
    "        # Get genre\n",
    "        genre = medium_tracks.loc[track_id, ('track', 'genre_top')]\n",
    "        \n",
    "        # Skip if genre not in our list\n",
    "        if pd.isna(genre) or genre not in GENRE_LABELS:\n",
    "            continue\n",
    "        \n",
    "        # Create file path: fma_medium/000/000001.mp3\n",
    "        folder = str(track_id).zfill(6)[:3]\n",
    "        file_path = audio_dir / folder / f\"{str(track_id).zfill(6)}.mp3\"\n",
    "        \n",
    "        # Check if file exists\n",
    "        if file_path.exists():\n",
    "            label_idx = GENRE_LABELS.index(genre)\n",
    "            samples.append({\n",
    "                'track_id': track_id,\n",
    "                'path': str(file_path),\n",
    "                'genre': genre,\n",
    "                'label_idx': label_idx\n",
    "            })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "dataset_df = pd.DataFrame(samples)\n",
    "\n",
    "print(f\"\\n✓ Created dataset with {len(dataset_df)} samples\")\n",
    "print(f\"\\nGenre distribution:\")\n",
    "print(dataset_df['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1f034",
   "metadata": {},
   "source": [
    "## 4. Create Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e3a48be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13192\n",
      "Validation samples: 3298\n",
      "\n",
      "Training set genre distribution:\n",
      "genre\n",
      "Rock              4883\n",
      "Electronic        4251\n",
      "Experimental      1001\n",
      "Hip-Hop            961\n",
      "Classical          495\n",
      "Folk               415\n",
      "Jazz               307\n",
      "Instrumental       280\n",
      "Pop                149\n",
      "Country            143\n",
      "Soul-RnB           123\n",
      "Spoken              94\n",
      "Blues               59\n",
      "Easy Listening      17\n",
      "International       14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set genre distribution:\n",
      "genre\n",
      "Rock              1220\n",
      "Electronic        1063\n",
      "Experimental       250\n",
      "Hip-Hop            240\n",
      "Classical          124\n",
      "Folk               104\n",
      "Jazz                77\n",
      "Instrumental        70\n",
      "Pop                 37\n",
      "Country             35\n",
      "Soul-RnB            31\n",
      "Spoken              24\n",
      "Blues               15\n",
      "Easy Listening       4\n",
      "International        4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split dataset with stratification to maintain genre balance\n",
    "train_df, val_df = train_test_split(\n",
    "    dataset_df,\n",
    "    test_size=0.2,\n",
    "    stratify=dataset_df['label_idx'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "print(\"\\nTraining set genre distribution:\")\n",
    "print(train_df['genre'].value_counts())\n",
    "\n",
    "print(\"\\nValidation set genre distribution:\")\n",
    "print(val_df['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6db9e",
   "metadata": {},
   "source": [
    "## 5. Define AudioDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81c02ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AudioDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Dataset class for loading and preprocessing audio files\"\"\"\n",
    "    def __init__(self, file_list, labels, augment=False):\n",
    "        self.file_list = file_list\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess audio to log-mel spectrogram\n",
    "        spec = load_and_preprocess_audio(self.file_list[idx])\n",
    "        \n",
    "        # Apply augmentation if training\n",
    "        if self.augment:\n",
    "            spec = spec_augment(spec)\n",
    "        \n",
    "        # Convert to tensor and add channel dimension\n",
    "        spec = torch.FloatTensor(spec).unsqueeze(0)  # (1, n_mels, time_frames)\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        \n",
    "        return spec, label\n",
    "\n",
    "print(\"✓ AudioDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20681b",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66066cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created DataLoaders\n",
      "Training batches: 413\n",
      "Validation batches: 104\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = AudioDataset(\n",
    "    train_df['path'].tolist(),\n",
    "    train_df['label_idx'].tolist(),\n",
    "    augment=True  # Apply augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = AudioDataset(\n",
    "    val_df['path'].tolist(),\n",
    "    val_df['label_idx'].tolist(),\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✓ Created DataLoaders\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67392718",
   "metadata": {},
   "source": [
    "## 7. Initialize Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ed9800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SELECTED DEVICE: cpu (CUDA not available)\n",
      "✓ Model created with 423760 parameters\n",
      "✓ Training setup complete\n",
      "✓ Training setup complete\n"
     ]
    }
   ],
   "source": [
    "# Setup device - Automatically select best GPU\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} CUDA device(s)\")\n",
    "    \n",
    "    # List all GPUs and find the best one\n",
    "    best_device = 0\n",
    "    max_memory = 0\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory / 1024**3:.2f} GB)\")\n",
    "        \n",
    "        # Select GPU with most memory (usually the dedicated GPU)\n",
    "        if gpu_memory > max_memory:\n",
    "            max_memory = gpu_memory\n",
    "            best_device = i\n",
    "    \n",
    "    device = torch.device(f\"cuda:{best_device}\")\n",
    "    print(f\"\\n✓ SELECTED BEST GPU:\")\n",
    "    print(f\"  Device: cuda:{best_device}\")\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(best_device)}\")\n",
    "    print(f\"  GPU Memory: {max_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"✓ SELECTED DEVICE: {device} (CUDA not available)\")\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    n_mels=MODEL_CONFIG[\"input_shape\"][0],\n",
    "    n_classes=MODEL_CONFIG[\"n_classes\"]\n",
    ").to(device)\n",
    "\n",
    "print(f\"✓ Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=TRAINING_CONFIG[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "print(\"✓ Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d1d7c",
   "metadata": {},
   "source": [
    "## 8. Define Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84799e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for specs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(specs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for specs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec56d0",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a823738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "Training on 13192 samples, validating on 3298 samples\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 56/413 [07:13<41:53,  7.04s/it] c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\src\\preprocessing.py:25: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(\n",
      "c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\src\\preprocessing.py:25: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(\n",
      "c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_medium\\098\\098559.mp3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 60/413 [07:42<41:08,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_medium\\098\\098558.mp3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 169/413 [21:16<30:10,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_medium\\001\\001486.mp3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 190/413 [23:55<28:08,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load c:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep-learning-lab-aman\\PROJECT 1\\Music-classification-with-FMA-MEDIUM\\data\\fma_medium\\105\\105247.mp3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 207/413 [25:49<22:13,  6.47s/it]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    "print(f\"Starting training for {TRAINING_CONFIG['epochs']} epochs...\")\n",
    "print(f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG['epochs']):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{TRAINING_CONFIG['epochs']}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'models/best_model.pt')\n",
    "        print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d70919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(epochs_range, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(epochs_range, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves saved to models/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543eff2",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and display final results\n",
    "checkpoint = torch.load('models/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best Epoch: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Best Validation Accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"Best Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"Total Epochs Trained: {len(history['train_loss'])}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nMODEL ARCHITECTURE:\")\n",
    "print(\"-\" * 70)\n",
    "print(model)\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\n✓ Model saved to: models/best_model.pt\")\n",
    "print(f\"✓ Ready to use with GUI application!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c5de2",
   "metadata": {},
   "source": [
    "## 11. Final Evaluation & Model Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
